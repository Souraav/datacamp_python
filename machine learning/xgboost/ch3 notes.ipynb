{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Data prep\n",
    "ames = pd.read_csv(\"https://assets.datacamp.com/production/course_3679/datasets/ames_unprocessed_data.csv\")\n",
    "ames.iloc[1]\n",
    "\n",
    "# Last col is our predictor\n",
    "X, y =  ames.iloc[:,8:-2], ames.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boosting_rounds          rmse\n",
      "0                    5  53743.005208\n",
      "1                   10  36522.906250\n",
      "2                   15  34392.015625\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix,\n",
    "                        params=params,\n",
    "                        nfold=3,\n",
    "                        num_boost_round=curr_num_rounds,\n",
    "                        metrics=\"rmse\",\n",
    "                        as_pandas=True,\n",
    "                        seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std\n",
      "0    142892.885417     236.361461    142480.005208      490.910858\n",
      "1    104754.695312     848.913524    104181.117187      247.417632\n",
      "2     78863.869792    1105.393938     77528.343750      147.678522\n",
      "3     61609.513021    1630.284897     59411.843750      228.173655\n",
      "4     50341.221354    1679.488102     46907.351563      231.686642\n",
      "5     42951.391927    1376.968977     38602.959636      197.224172\n",
      "6     38632.955729    1021.952690     33081.843750      118.713353\n",
      "7     35967.014323     782.869883     29480.868489      140.258307\n",
      "8     34292.915364     488.589550     27017.141276      235.715252\n",
      "9     33306.989583     205.749927     25221.238932      181.201290\n",
      "10    32677.056640      77.830031     23917.205078       68.229945\n",
      "11    32414.880209     153.244428     23147.573568       46.121184\n",
      "12    32082.016927     395.420825     22413.623047      125.869531\n",
      "13    31919.809896     395.424739     21944.130208      188.008559\n",
      "14    31872.845052     394.741073     21533.210286      274.412461\n",
      "15    31719.640625     471.181280     21180.413411      304.369144\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(dtrain = housing_dmatrix,\n",
    "                    params = params,\n",
    "                    num_boost_round = 50,\n",
    "                    early_stopping_rounds = 10,\n",
    "                    metrics = \"rmse\",\n",
    "                    as_pandas = True,\n",
    "                    seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eta      best_rmse\n",
      "0  0.001  195751.286458\n",
      "1  0.010  180072.510417\n",
      "2  0.100   80951.346354\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix,\n",
    "                    params=params,\n",
    "                    nfold=3,\n",
    "                    num_boost_round=10,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"rmse\",\n",
    "                    as_pandas=True,\n",
    "                    seed=123)\n",
    "\n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   max_depth     best_rmse\n",
      "0          2  41615.080078\n",
      "1          5  37130.722656\n",
      "2         10  37970.410157\n",
      "3         20  38235.009765\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:linear\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2, 5, 10, 20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix,\n",
    "                    params=params,\n",
    "                    nfold=2,\n",
    "                    num_boost_round=10,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"rmse\",\n",
    "                    as_pandas=True,\n",
    "                    seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   colsample_bytree     best_rmse\n",
      "0               0.1  59161.455079\n",
      "1               0.5  40903.736328\n",
      "2               0.8  39494.525391\n",
      "3               1.0  39629.414062\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values: colsample_bytree_vals\n",
    "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params[\"colsample_bytree\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix,\n",
    "                        params=params,\n",
    "                        nfold=2,\n",
    "                        num_boost_round=10,\n",
    "                        early_stopping_rounds=5,\n",
    "                        metrics=\"rmse\",\n",
    "                        as_pandas=True,\n",
    "                        seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n",
      "Best parameters found:  {'colsample_bytree': 0.7, 'max_depth': 5, 'n_estimators': 50}\n",
      "Lowest RMSE found:  32397.6579836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "# load extra modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator = gbm,\n",
    "                        param_grid = gbm_param_grid,\n",
    "                        scoring = \"neg_mean_squared_error\",\n",
    "                        cv = 4,\n",
    "                        verbose = 1)\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
      "Best parameters found:  {'n_estimators': 25, 'max_depth': 5}\n",
      "Lowest RMSE found:  37479.1964196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    1.2s finished\n"
     ]
    }
   ],
   "source": [
    "# load extra modules\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Create the parameter grid: gbm_param_grid \n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': range(2, 12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(estimator = gbm,\n",
    "                               param_distributions = gbm_param_grid,\n",
    "                               scoring = \"neg_mean_squared_error\",\n",
    "                               n_iter = 5,\n",
    "                               cv = 4,\n",
    "                               verbose = 1)\n",
    "\n",
    "# Fit randomized_mse to the data\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
