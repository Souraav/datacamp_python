{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_data = {'numeric': np.array([ -1.08563060e+01,   9.97345447e+00,   2.82978498e+00,\n",
    "         -1.50629471e+01,  -5.78600252e+00,   1.65143654e+01,\n",
    "         -2.42667924e+01,  -4.28912629e+00,   1.26593626e+01,\n",
    "         -8.66740402e+00,  -6.78886152e+00,  -9.47089689e-01,\n",
    "          1.49138963e+01,  -6.38901997e+00,  -4.43981960e+00,\n",
    "         -4.34351276e+00,   2.20593008e+01,   2.18678609e+01,\n",
    "          1.00405390e+01,   3.86186399e+00,   7.37368576e+00,\n",
    "          1.49073203e+01,  -9.35833868e+00,   1.17582904e+01,\n",
    "         -1.25388067e+01,  -6.37751502e+00,   9.07105196e+00,\n",
    "         -1.42868070e+01,  -1.40068720e+00,  -8.61754896e+00,\n",
    "         -2.55619371e+00,  -2.79858911e+01,  -1.77153310e+01,\n",
    "         -6.99877235e+00,   9.27462432e+00,  -1.73635683e+00,\n",
    "          2.84591590e-02,   6.88222711e+00,  -8.79536343e+00,\n",
    "          2.83627324e+00,  -8.05366518e+00,  -1.72766949e+01,\n",
    "         -3.90899794e+00,   5.73805862e+00,   3.38589051e+00,\n",
    "         -1.18304945e-01,   2.39236527e+01,   4.12912160e+00,\n",
    "          9.78736006e+00,   2.23814334e+01,  -1.29408532e+01,\n",
    "         -1.03878821e+01,   1.74371223e+01,  -7.98062735e+00,\n",
    "          2.96832303e-01,   1.06931597e+01,   8.90706391e+00,\n",
    "          1.75488618e+01,   1.49564414e+01,   1.06939267e+01,\n",
    "         -7.72708714e+00,   7.94862668e+00,   3.14271995e+00,\n",
    "         -1.32626546e+01,   1.41729905e+01,   8.07236535e+00,\n",
    "          4.54900806e-01,  -2.33092061e+00,  -1.19830114e+01,\n",
    "          1.99524074e+00,   4.68439119e+00,  -8.31154984e+00,\n",
    "          1.16220405e+01,  -1.09720305e+01,  -2.12310035e+01,\n",
    "          1.03972709e+01,  -4.03366038e+00,  -1.26029585e+00,\n",
    "         -8.37516723e+00,  -1.60596276e+01,   1.25523737e+01,\n",
    "         -6.88868984e+00,   1.66095249e+01,   8.07308186e+00,\n",
    "         -3.14758147e+00,  -1.08590240e+01,  -7.32461987e+00,\n",
    "         -1.21252313e+01,   2.08711336e+01,   1.64441230e+00,\n",
    "          1.15020554e+01,  -1.26735205e+01,   1.81035130e+00,\n",
    "          1.17786194e+01,  -3.35010762e+00,   1.03111446e+01,\n",
    "         -1.08456791e+01,  -1.36347154e+01,   3.79400612e+00,\n",
    "         -3.79176435e+00,   6.42054689e+00,  -1.97788793e+01,\n",
    "          7.12264635e+00,   2.59830393e+01,  -2.46259814e-01,\n",
    "          3.41421289e-01,   1.79549485e+00,  -1.86197571e+01,\n",
    "          4.26146640e+00,  -1.60540974e+01,  -4.27679598e+00,\n",
    "          1.24286955e+01,  -7.35216956e+00,   5.01248990e+00,\n",
    "          1.01273905e+01,   2.78740856e+00,  -1.37094847e+01,\n",
    "         -3.32475275e+00,   1.95941134e+01,  -2.02504576e+01,\n",
    "         -2.75786014e+00,  -5.52108071e+00,   1.20747363e+00,\n",
    "          7.48215617e+00,   1.60869097e+01,  -2.70232392e+00,\n",
    "          8.12341330e+00,   4.99740145e+00,   4.74347298e+00,\n",
    "         -5.63923932e+00,  -9.97321469e+00,  -1.10004311e+01,\n",
    "         -7.56437209e+00,   3.21686576e+00,   7.60949393e+00,\n",
    "          3.23468848e+00,  -5.48955096e+00,   1.80597011e+01,\n",
    "          1.51886562e+01,  -3.54000113e+00,  -8.23431406e+00,\n",
    "          1.30214954e+00,   1.26729865e+01,   3.32764977e+00,\n",
    "          5.56548705e+00,  -2.12080122e+00,   4.56270895e+00,\n",
    "          1.54454445e+01,  -2.39668781e+00,   1.43307733e+00,\n",
    "          2.53816477e+00,   2.83725356e+00,  -1.41188888e+01,\n",
    "         -1.87686866e+01,  -1.01965507e+01,   1.67942295e+00,\n",
    "          5.53856166e+00,  -5.30674560e+00,   1.37725748e+01,\n",
    "         -1.43175974e+00,   2.03159982e-01,  -1.93963871e+00,\n",
    "          1.34026793e+00,   7.04474074e+00,   6.65653438e+00,\n",
    "         -8.98422941e+00,   1.52366378e+01,  -1.09502646e+01,\n",
    "          7.92270141e-01,  -2.74396574e+00,  -1.04899168e+01,\n",
    "         -7.51205880e-01,  -7.40813773e+00,   7.29072433e-01,\n",
    "          4.03085961e+00,   1.47192937e+01,   3.07384219e+00,\n",
    "         -6.11225340e+00,  -3.91619811e+00,   1.39978106e+00,\n",
    "          9.34608295e-01,   1.45958927e+01,   1.39535293e+01,\n",
    "         -3.58935926e+00,  -5.48642128e+00,  -2.55705460e+01,\n",
    "         -5.48920413e+00,  -9.78057706e+00,  -3.54824458e+00,\n",
    "          3.91584242e+00,   1.77192329e+00,  -2.99680070e-01,\n",
    "          1.99582111e+00,  -1.26117773e+00,   1.97018933e+00,\n",
    "         -3.23105501e+01,  -2.69293490e+00,  -1.10850721e+00,\n",
    "         -3.41261716e+00,  -2.17946262e+00,   7.03310118e+00,\n",
    "         -5.98105331e+00,   2.20070210e+01,   6.88296930e+00,\n",
    "         -6.30725091e-02,  -2.06662303e+00,  -8.65222864e-01,\n",
    "         -9.15307070e+00,  -9.52025393e-01,   2.78683517e+00,\n",
    "          5.79541616e+00,   5.79689779e+00,  -2.74877547e+00,\n",
    "         -1.41608225e+01,  -6.69102626e+00,   1.61219304e+01,\n",
    "          8.96058313e+00,   3.69619586e+00,  -7.61294245e+00,\n",
    "          3.64515497e-02,  -1.25566869e+01,  -5.51936876e+00,\n",
    "         -2.45203342e+00,  -3.61639932e+00,   9.56601931e+00,\n",
    "         -1.41872591e+01,  -8.65432272e+00,  -1.37468797e+01,\n",
    "         -1.23735321e+01,   1.24055896e+00,  -1.60044053e+01,\n",
    "          7.53868779e+00,  -2.46815777e+00,   6.87883325e-01,\n",
    "          3.22576738e+00,  -4.34166520e+00,   1.03247972e+01,\n",
    "         -1.94342727e+00,   5.94070255e+00,  -1.99112383e+00,\n",
    "          2.90874387e+00,   2.79662670e+00,   2.49969939e+00,\n",
    "         -9.74307850e+00,   4.35875771e+00,  -3.18956989e+00,\n",
    "          6.30488030e+00,  -2.15249344e+01,  -1.46511622e+01,\n",
    "          3.63445562e+00,   1.86292759e+01,   8.35058747e+00,\n",
    "         -6.82450934e+00,  -1.69205242e+01,   7.42686221e+00,\n",
    "         -8.05832220e-01,   5.90704218e+00,   1.15298722e+00,\n",
    "          2.96429251e-01,   2.95862545e+01,  -6.12996105e-02,\n",
    "         -1.59245209e+00,  -1.21448675e+00,  -5.83536717e+00,\n",
    "          9.90132895e+00,  -3.53754334e+00,   6.35942706e+00,\n",
    "          2.84603030e+00,   1.21898586e+01,   4.20179597e+00,\n",
    "         -1.21338478e+01,  -1.32648779e+01,   1.40836915e+01,\n",
    "         -6.08710803e+00,  -1.32060258e+01,  -6.69618600e+00,\n",
    "          1.26462529e+01,  -1.42021300e+01,  -8.66495204e+00,\n",
    "         -6.66807509e+00,  -1.25118987e+01,  -1.18432734e+01,\n",
    "         -1.51810798e+01,  -4.61187413e+00,  -3.54908832e+00,\n",
    "         -6.82538154e+00,  -1.65369784e+01,   1.25333595e+01,\n",
    "         -1.32907883e+01,   2.78033712e+00,  -1.07476659e+01,\n",
    "          6.68316867e+00,   9.55832355e+00,  -8.77613587e+00,\n",
    "         -1.92371573e+01,   6.95787319e+00,   1.87580055e+01,\n",
    "          4.15694540e+00,   1.60544421e+00,   8.19760610e+00,\n",
    "          7.65054846e+00,  -8.28988834e+00,  -6.59151311e+00,\n",
    "          6.11123550e+00,  -1.44013347e+00,   1.31660560e+01,\n",
    "         -7.04342147e+00,   7.50609917e+00,   3.42637981e+00,\n",
    "         -1.26437564e+00,   1.17591077e+01,   6.80071533e+00,\n",
    "         -1.00496715e+01,   6.40218680e+00,   1.37499063e+01,\n",
    "         -1.30444689e+00,  -2.48655850e+00,  -6.69647148e+00,\n",
    "         -1.36038857e-01,   6.86200686e+00,  -8.17668300e+00,\n",
    "         -1.34635756e+01,  -3.75749911e+00,  -1.37972498e+01,\n",
    "          5.23218441e+00,  -4.26689770e+00,  -1.75540184e+01,\n",
    "         -3.48607515e+00,  -1.92614986e+00,   4.49135613e+00,\n",
    "         -1.45363543e+00,   1.86872646e+01,  -5.18703852e+00,\n",
    "         -6.23985486e-01,  -1.02910614e+00,  -2.82628384e+00,\n",
    "          1.42425585e+00,   5.41231297e+00,   1.34009870e+01,\n",
    "         -1.56925613e+01,  -5.10342874e+00,  -4.47771425e+00,\n",
    "          9.37850297e+00,  -3.56663061e+00,  -1.89517559e+01,\n",
    "          8.77304639e-01,  -3.36892315e-01,   1.79751566e+00,\n",
    "         -1.04016288e+01,   1.71903468e+01,  -3.23859784e+00,\n",
    "         -1.88296858e+00,  -9.00008570e+00,  -9.31002004e+00,\n",
    "         -1.22273696e+01,  -3.93310852e+00,  -9.57581839e+00,\n",
    "          2.05646734e+01,  -1.88849238e+01,  -1.12833055e+01,\n",
    "         -4.01414428e+00,   6.73491278e+00,  -4.13756881e+00,\n",
    "          6.75963393e+00,  -9.86803889e+00,   5.92911339e-01,\n",
    "          1.74404109e+01,  -9.67744396e+00,   4.19567676e+00,\n",
    "          2.06927519e+00,  -2.25153499e+01,  -5.88970546e+00,\n",
    "          1.13115191e+01,   1.35077672e+00,  -1.21226896e+01,\n",
    "          6.90777273e+00,  -4.79122945e+00,   3.60050728e+00,\n",
    "          3.76920114e+00,  -1.11869560e+01,   7.89828035e+00,\n",
    "         -1.00750880e+01,  -1.30578587e+01,  -8.82828988e+00,\n",
    "         -3.46090034e+00,   1.09403156e+00,  -7.72583732e+00,\n",
    "          7.44819312e+00,   2.51464184e+00,  -6.94798215e+00,\n",
    "          8.88992918e+00,   1.16106836e+01,  -9.86846968e-01,\n",
    "         -2.14983045e+00,  -1.77377135e+01,  -4.07512592e+00,\n",
    "         -2.91506713e+00,   2.45379407e+00,  -1.68426432e+00,\n",
    "          2.44026938e+00,   1.53409029e+01,  -5.29914099e+00,\n",
    "         -4.90972283e+00,  -1.30916531e+01,  -8.66046674e-02,\n",
    "          9.76812982e+00,  -1.75107035e+01,  -6.65856967e+00,\n",
    "          3.59405025e-01,   8.50102884e+00,   3.82870239e+00,\n",
    "          3.25463628e+00,  -2.24312786e+00,   4.81874257e+00,\n",
    "          1.01430388e+01,  -1.70899178e+01,   7.28535401e+00,\n",
    "         -9.87598059e-01,  -5.29988864e+00,  -2.44307579e+01,\n",
    "         -1.38035132e+01,   1.69805899e+01,  -6.88548532e+00,\n",
    "         -1.08476877e+01,  -4.56425956e+00,  -7.45147222e+00,\n",
    "          1.24358633e+00,   1.51697368e+01,  -5.86615951e+00,\n",
    "          1.54290051e+00,  -1.14723714e+01,   1.52016644e+01,\n",
    "          1.89043442e+00,  -1.08181912e+01,   9.04306220e+00,\n",
    "          1.19998765e+01,   1.61456363e+00,  -1.63939697e+01,\n",
    "          1.78516881e+01,   3.10122521e+00,   1.17045430e+01,\n",
    "         -5.50893205e+00,  -3.98623925e+00,   1.10453109e+00,\n",
    "          1.11340168e+01,   1.42990028e+00,  -1.25683381e+01,\n",
    "          2.61989197e+00,   1.62953419e+01,   6.58062825e-01,\n",
    "          1.29385524e+00,  -1.26254374e+01,  -1.40298909e+01,\n",
    "         -6.37692139e+00,  -3.26517647e+00,   1.04060735e+00,\n",
    "          1.65956974e+00,   1.60190765e+01,   5.86874384e-01,\n",
    "          1.06442269e+01,  -3.93293346e-01,   1.44890361e+01,\n",
    "         -1.87039748e+01,  -5.98731767e+00,   9.83033396e+00,\n",
    "         -1.71596005e+00,   9.31529613e+00,   3.85066103e+00,\n",
    "          9.45876984e+00,   6.13067728e+00,   6.73648934e+00,\n",
    "          1.49245539e+01,   9.86474019e+00,   9.93806707e+00,\n",
    "          2.04187588e-01,  -5.81850210e+00,  -6.59560088e+00,\n",
    "          7.50944597e+00,  -2.43846056e+01,  -1.30717819e+01,\n",
    "         -9.63253686e+00,  -2.47621379e+00,  -2.63143804e+01,\n",
    "          1.25872130e+01,   6.00175236e+00,   1.98458240e+01,\n",
    "          1.06221447e+01,   1.03037933e+01,   2.25067603e+01,\n",
    "         -5.70958087e+00,  -1.50345840e+01,  -2.32357396e+00,\n",
    "         -8.24992054e+00,  -2.56845361e+00,  -1.05021899e+01,\n",
    "          5.67880067e+00,   5.12982850e+00,  -2.69225906e-01,\n",
    "          3.11581503e+00,  -1.42050727e+00,   7.52033668e+00,\n",
    "          6.96407495e-01,  -2.65445625e+00,   9.29584470e+00,\n",
    "          1.26082127e+01,   1.33745383e+01,  -9.90274689e+00,\n",
    "         -5.34447208e+00,  -1.70149609e+01,  -2.38565018e+00,\n",
    "          4.16123814e+00,  -1.32801175e+00,   1.93085087e+01,\n",
    "         -4.05843375e+00,  -1.08850856e+01,   5.68504207e+00,\n",
    "          1.49933828e+00,   5.49642596e+00,   3.03787931e+00,\n",
    "          2.43765761e+00,  -2.12229888e+01,  -7.26114028e+00,\n",
    "         -5.90324918e+00,  -3.47964338e+00,  -4.48391638e+00,\n",
    "          4.12818762e+00,   6.00882509e+00,  -1.13164093e+01,\n",
    "          6.86804407e+00,   5.85768139e+00,   4.65770066e+00,\n",
    "         -1.41789449e+01,   4.40582300e+00,   8.25954195e-01,\n",
    "          9.60880075e+00,   1.78435680e+01,   1.58806789e+01,\n",
    "          1.12644172e+00,  -1.87469173e-02,   6.31948284e+00,\n",
    "         -1.11503995e+01,  -1.45068314e+01,  -4.72530592e+00,\n",
    "          1.22056891e+01,  -3.31128298e+00,   1.50258091e+01,\n",
    "         -2.78811288e+01,  -1.58720643e+01,  -1.23688733e+00,\n",
    "          8.88194541e+00,  -6.96230987e-01,  -9.56527445e-01,\n",
    "         -1.21741495e+00,  -1.76289773e+01,   1.15806887e+01,\n",
    "         -6.82764720e+00,   1.08953266e+01,  -7.05234180e-01,\n",
    "          1.06816039e+01,   3.43257981e+00,  -1.06674491e+00,\n",
    "          2.26261647e+00,  -1.47297761e+01,   6.02932717e-02,\n",
    "          1.71604032e+01,   1.30819356e+01,  -9.85023583e+00,\n",
    "          5.05685851e+00,   2.47979885e+00,   8.17188280e+00,\n",
    "          9.41528855e-01,  -2.33503993e+00,   1.33063078e+01,\n",
    "          2.16396568e+00,   1.25546432e+01,   9.74386203e+00,\n",
    "         -3.24487242e+00,  -1.67000274e+00,   1.92983243e+01,\n",
    "          1.91482325e+00,  -7.58628423e+00,  -1.20004328e+01,\n",
    "          9.74132435e+00,   1.60293721e+00,  -7.92091951e+00,\n",
    "          6.74584831e+00,   1.59748695e+00,  -1.66047507e+01,\n",
    "          5.88685555e+00,   1.33319070e+01,   2.55985023e+01,\n",
    "          3.49025641e-01,   2.32649658e+00,   1.60659681e+01,\n",
    "          1.68722267e+00,   2.75341863e+00,  -6.30618391e+00,\n",
    "         -1.39437511e+01,   9.12687936e+00,  -1.27357024e+01,\n",
    "          1.14065624e+01,  -7.88165559e+00,   2.65233958e+00,\n",
    "         -3.72271731e+00,   1.17460024e+01,   3.00846402e+00,\n",
    "          1.95909535e+01,  -1.08367768e+01,   4.13863082e+00,\n",
    "          4.73418809e-01,   2.79943571e+00,   1.59592117e+01,\n",
    "          5.85318030e+00,  -1.14752541e+01,   5.33510818e+00,\n",
    "         -4.15619376e+00,  -4.73354970e+00,   1.04585733e+01,\n",
    "         -6.02339983e+00,  -3.79730328e+00,   3.24072544e-01,\n",
    "          9.46185897e+00,   5.81589803e+00,  -2.34414912e+00,\n",
    "         -2.72176466e+00,  -1.16012966e+01,  -7.59835089e+00,\n",
    "         -6.54381086e+00,  -1.49290942e+00,   1.98676304e+01,\n",
    "         -6.75469434e+00,  -2.95829068e+00,  -2.05239840e+01,\n",
    "          6.00571094e+00,  -1.57744879e+01,  -9.06589522e+00,\n",
    "          1.04233537e+01,  -2.10492843e+01,   9.59204124e-01,\n",
    "          8.00929647e+00,   1.52544424e+01,  -9.20093108e+00,\n",
    "         -6.24024850e+00,   1.80443632e+01,  -2.50601446e-01,\n",
    "         -7.36263934e+00,  -7.17485837e+00,  -9.72556324e+00,\n",
    "         -4.39672205e+00,   1.08822048e+01,   8.87766069e+00,\n",
    "         -1.17427030e+01,   2.14586075e+00,  -4.02159735e+00,\n",
    "          7.82440404e+00,  -1.56359727e+00,   3.71524682e+00,\n",
    "         -1.58605207e+00,  -2.18708593e+01,   4.70024454e-01,\n",
    "         -1.27174695e+01,   7.47128412e+00,  -6.79827892e+00,\n",
    "         -6.05012656e+00,  -8.90282501e-02,   5.77793954e+00,\n",
    "         -1.69743818e+00,   1.64218277e+01,   1.64881786e+01,\n",
    "          1.71546859e+01,  -2.34318725e+00,  -1.48195846e+01,\n",
    "          1.22007851e+01,  -4.16345285e+00,   3.81663120e-01,\n",
    "          6.28320103e+00,  -1.60443640e+01,   1.62912121e+01,\n",
    "          8.74962329e+00,   3.52271582e+00,   8.90916750e+00,\n",
    "          5.56952539e+00,   1.16828116e+00,  -3.07774322e+00,\n",
    "         -7.39142274e+00,   2.87214281e+00,   3.61203060e+00,\n",
    "          1.08319880e+01,   2.00660009e+00,  -8.53201516e+00,\n",
    "         -1.57623285e+01,  -1.27562162e+00,  -2.74583767e-01,\n",
    "         -9.80915828e-01,  -1.53051290e+01,   1.07446514e+01,\n",
    "          1.07262975e+01,  -8.23285165e+00,   8.05982112e+00,\n",
    "          9.52830862e+00,   3.84470766e-01,  -9.48498626e+00,\n",
    "          1.61338154e+01,   3.80921434e+00,  -2.51037455e+00,\n",
    "          7.72845396e+00,   1.62220849e+01,  -4.97160147e+00,\n",
    "          6.91422545e+00,   1.94316017e+01,  -1.11656483e+01,\n",
    "         -5.49253589e+00,  -5.97855081e+00,  -1.58327137e+00,\n",
    "         -2.09973736e+00,   3.00795439e+00,  -8.20375520e+00,\n",
    "         -6.22663489e+00,   3.01290484e+00,   1.54911860e+00,\n",
    "          8.74079809e+00,   4.09355025e+00,   7.71846636e+00,\n",
    "         -1.14295204e+01,  -3.83416311e+00,   1.84837995e+01,\n",
    "          3.92078047e+00,   7.47392962e+00,   2.72734737e+00,\n",
    "          4.25335857e+00,  -2.30904052e+00,   3.57157922e+01,\n",
    "         -3.96155922e+00,  -3.83212177e-01,  -2.42483633e+01,\n",
    "          1.10626666e+01,  -5.52252950e+00,  -6.07941060e-01,\n",
    "         -5.28040551e+00,  -1.38682945e+00,  -2.84247098e+00,\n",
    "         -4.30417758e-01,  -5.28014827e-01,  -1.54205311e+00,\n",
    "         -1.67877227e+00,   1.30851845e+01,   8.81416920e+00,\n",
    "          8.87442318e+00,  -7.32448881e+00,   1.25682083e+01,\n",
    "          1.13164451e+01,  -5.53404329e+00,   1.52385202e+01,\n",
    "         -6.83225943e+00,  -1.84203184e+01,   1.10152599e+01,\n",
    "         -1.19518780e+00,   9.76921565e+00,  -7.54202163e+00,\n",
    "         -1.26177001e+01,   1.23187179e+01,  -1.40726856e+01,\n",
    "         -1.56103051e+01,  -1.92558689e+01,  -2.79447230e+01,\n",
    "         -1.03910438e+01,  -2.16940796e+01,  -3.56054846e-01,\n",
    "         -2.23641735e+01,  -1.09427342e+01,   2.33206200e+01,\n",
    "          3.45124241e+00,   5.28056754e-01,  -1.62075198e+01,\n",
    "         -2.45597666e+00,   4.81085631e-01,  -1.60518080e+01,\n",
    "          1.26180681e+00,   1.19123246e+01,  -3.48258119e+00,\n",
    "         -1.97236368e+00,  -2.04209642e+01,  -1.39923387e+01,\n",
    "          1.05769510e+01,  -6.61550647e+00,  -9.29711634e+00,\n",
    "         -6.92583116e+00,  -5.74661270e-01,  -1.15841591e+01,\n",
    "          1.26516506e+00,  -1.36000768e+01,   7.74000002e+00,\n",
    "         -1.05705578e+01,   1.32026832e+01,  -1.00326528e-01,\n",
    "         -8.45644428e+00,   9.11460610e+00,  -1.37449688e+01,\n",
    "         -5.47065645e+00,  -7.55266106e-04,  -1.21166803e+00,\n",
    "         -2.00858547e+01,  -9.20646543e+00,   1.68234342e+00,\n",
    "         -1.31989156e+01,   1.26642930e+01,   4.95180889e+00,\n",
    "         -5.14240391e+00,  -2.20292465e+00,   1.86156412e+01,\n",
    "          9.35988451e+00,   3.80219145e+00,  -1.41551877e+01,\n",
    "          1.62961132e+01,   1.05240107e+01,  -1.48405388e+00,\n",
    "         -5.49698069e+00,  -1.87903939e+00,  -1.20193668e+01,\n",
    "         -4.70785558e+00,   7.63160514e+00,  -1.80762128e+01,\n",
    "         -3.14074374e+00,   1.13755973e+00,   1.03568037e+00,\n",
    "         -1.17893695e+01,  -1.18215289e+01,   1.08916538e+01,\n",
    "         -1.22452909e+01,   1.00865096e+01,  -4.82365315e+00,\n",
    "          1.07979635e+01,  -4.21078505e+00,  -1.16647132e+01,\n",
    "          8.56554856e+00,  -1.73912222e-01,   1.44857659e+01,\n",
    "          8.92200085e+00,  -2.29426629e+00,  -4.49667602e+00,\n",
    "          2.33723433e-01,   1.90210018e+00,  -8.81748527e+00,\n",
    "          8.41939573e+00,  -3.97363492e+00,  -4.23027745e+00,\n",
    "         -5.40688337e+00,   2.31017267e+00,  -6.92052602e+00,\n",
    "          1.34970110e+00,   2.76660307e+01,  -5.36094601e-01,\n",
    "         -4.34004738e+00,  -1.66768923e+01,   5.02219248e-01,\n",
    "         -1.10923094e+01,  -3.75558119e+00,   1.51607594e+00,\n",
    "         -1.73098945e+01,   1.57462752e+00,   3.04515175e+00,\n",
    "         -1.29710002e+01,  -3.92309192e+00,  -1.83066636e+01,\n",
    "          1.57550094e+01,   3.30563277e+00,  -1.79588501e+00,\n",
    "         -1.63435831e+00,   1.13144361e+01,  -9.41655519e-01,\n",
    "          3.30816771e+00,   1.51862956e+01,  -3.46167148e+00,\n",
    "         -1.09263532e+01,  -8.24500575e+00,   1.42866383e+01,\n",
    "          9.14283085e-01,  -5.02331288e+00,   9.73644380e+00,\n",
    "          9.97957386e+00,  -4.75647768e+00,  -9.71936837e+00,\n",
    "         -1.57052860e+01,  -1.79388892e+01,  -2.64986452e+00,\n",
    "         -8.93195947e+00,   1.85847441e+01,   5.85377547e-01,\n",
    "         -1.94214954e+01,   1.41872928e+01,   1.61710309e+00,\n",
    "          7.04979480e+00,   6.82034777e+00,   2.96556567e+00,\n",
    "          5.23342630e+00,   2.38760672e+00,  -1.10638591e+01,\n",
    "          3.66732198e+00,   1.02390550e+01,  -2.10056413e+00,\n",
    "          5.51302218e+00,   4.19589145e+00,   1.81565206e+01,\n",
    "         -2.52750301e+00,  -2.92004163e+00,  -1.16931740e+00,\n",
    "         -1.02391075e+00,  -2.27261771e+01,  -6.42609841e+00,\n",
    "          2.99885067e+00,  -8.25651467e-02,  -7.99339154e+00,\n",
    "         -6.64779252e+00,  -3.55613128e+00,  -8.01571781e+00,\n",
    "         -5.13050610e+00,  -5.39390119e+00,   8.95370847e+00,\n",
    "          1.01639127e+01,   9.33585094e+00,   4.26701799e+00,\n",
    "         -7.08322484e+00,   9.59830450e+00,  -3.14250587e+00,\n",
    "          2.30522083e-01,   1.33822053e+01,   8.39928561e-01,\n",
    "          2.47284030e+00,  -1.41277949e+01,   4.87009294e+00,\n",
    "         -9.80006647e+00,   1.01193966e+01,  -1.84599177e+00,\n",
    "         -2.23616884e+01,  -3.58020103e+00,  -2.28034538e+00,\n",
    "          4.85475226e+00,   6.70512391e+00,  -3.27764245e+00,\n",
    "          1.01286819e+01,  -3.16705533e+01,  -7.13988998e+00,\n",
    "         -1.11236427e+01,  -1.25418351e+01,   9.59706371e+00,\n",
    "          8.29170399e+00,  -7.75770020e+00,   1.17805700e+01,\n",
    "          1.01466892e+00,  -4.21684101e+00,  -6.92922796e+00,\n",
    "         -7.78271726e+00,   4.72774857e+00,   6.50154901e+00,\n",
    "          2.38501212e+00,  -2.05021768e+01,   2.96358656e+00,\n",
    "          5.65396564e+00,  -6.69205605e+00,   4.32505429e-01,\n",
    "         -1.86388430e+01,  -1.22996906e+01,  -3.24235348e+00,\n",
    "         -3.09751144e+00,   3.51679372e+00,  -1.18692539e+01,\n",
    "         -3.41206065e+00,  -4.89779780e+00,   5.28010474e+00,\n",
    "          1.42104277e+01,   1.72092032e+01,  -1.56844005e+01,\n",
    "         -4.80141918e-01,  -1.11252931e+01,  -6.47449515e-01,\n",
    "          4.22919280e+00,   8.14908987e-01,  -4.90116988e-01,\n",
    "          1.48303917e+01,   7.20989392e+00,  -2.72654462e+00,\n",
    "          2.42113609e-01,   8.70897807e+00,   6.09790506e+00,\n",
    "         -4.25076104e+00,  -1.77524284e+01,  -1.18465749e+01,\n",
    "          1.45979225e+00,  -1.78652685e+01,  -1.52394498e+00,\n",
    "         -4.53569176e+00,   9.99252803e+00,  -1.31804382e+01,\n",
    "         -1.93176898e+01,  -4.19640742e+00,   6.34763132e+00,\n",
    "          1.06991860e+01,  -9.09327017e+00,   4.70263748e+00,\n",
    "         -1.11143045e+01]),\n",
    " 'text': np.array(['', 'foo', 'foo bar', '', 'foo bar', '', 'foo bar', 'foo', '',\n",
    "        'bar', 'foo bar', 'bar', '', 'bar foo', '', 'bar foo', 'foo', 'foo',\n",
    "        'bar', 'foo bar', 'bar', 'bar', '', 'bar foo', '', 'bar', 'foo', '',\n",
    "        '', 'foo bar', 'bar foo', 'foo bar', 'foo bar', '', 'foo bar', '',\n",
    "        'bar', 'foo bar', 'foo', 'foo bar', '', '', '', '', 'bar', 'bar',\n",
    "        'bar', 'bar', '', 'foo', 'foo', 'bar', 'foo bar', 'foo', 'foo bar',\n",
    "        'foo bar', 'foo bar', 'foo', 'bar foo', 'foo', 'bar foo', 'bar foo',\n",
    "        '', 'foo bar', 'foo bar', 'bar', '', 'bar foo', 'bar foo',\n",
    "        'foo bar', 'foo', 'foo bar', 'bar foo', 'foo bar', 'foo', 'foo bar',\n",
    "        'bar foo', 'bar', 'bar foo', 'foo bar', 'bar', 'bar foo', 'foo',\n",
    "        'foo', 'bar foo', 'foo', 'bar', '', 'bar foo', 'foo bar', 'bar foo',\n",
    "        'bar foo', 'foo', 'foo', 'foo bar', 'bar', 'bar', 'bar foo', 'foo',\n",
    "        'bar', '', 'foo', 'bar', 'foo', 'bar', 'foo', 'bar foo', 'bar',\n",
    "        'bar', 'foo', 'bar foo', 'bar foo', 'foo bar', 'foo bar', '',\n",
    "        'foo bar', 'bar', 'foo', '', 'foo bar', 'foo bar', '', 'foo bar',\n",
    "        'foo bar', 'foo bar', '', 'bar', '', '', 'foo bar', 'bar', 'foo',\n",
    "        'bar foo', 'foo', 'foo bar', 'foo', 'bar', 'bar', 'foo bar', 'bar',\n",
    "        'foo bar', 'foo bar', '', 'foo', '', 'foo bar', 'bar foo',\n",
    "        'bar foo', 'foo bar', 'bar foo', 'foo', '', 'foo', 'bar', '', '',\n",
    "        'foo', '', 'foo bar', 'foo', 'bar foo', 'foo', 'bar foo', 'foo',\n",
    "        'foo bar', 'bar', 'foo bar', 'foo bar', 'bar', 'bar foo', 'foo bar',\n",
    "        'bar', 'bar', 'foo', '', 'foo', '', 'foo bar', 'foo', 'foo bar',\n",
    "        'bar', 'foo', 'bar foo', 'bar', 'bar foo', 'bar foo', '', 'bar foo',\n",
    "        'foo', 'foo bar', 'bar foo', '', 'bar foo', 'foo bar', 'foo',\n",
    "        'bar foo', 'bar', 'foo bar', 'bar foo', 'foo bar', 'foo bar',\n",
    "        'foo bar', 'foo bar', '', '', '', 'foo bar', 'foo', 'bar', '',\n",
    "        'foo bar', '', 'bar', '', 'foo bar', '', 'foo', 'foo bar', 'foo',\n",
    "        'bar foo', 'bar foo', 'bar foo', 'bar', '', '', 'foo bar', 'foo',\n",
    "        'foo', 'foo bar', 'bar foo', 'foo', '', 'bar', 'foo', '', 'bar foo',\n",
    "        'bar', 'bar foo', '', 'bar foo', 'foo bar', '', 'bar foo',\n",
    "        'foo bar', 'foo bar', 'bar foo', 'bar foo', 'foo bar', '', '',\n",
    "        'bar foo', 'bar', 'foo', 'bar foo', 'foo bar', 'bar', 'bar', 'foo',\n",
    "        'foo', 'bar', '', 'foo bar', 'bar foo', 'bar', 'foo', 'foo', 'foo',\n",
    "        'bar', 'foo', '', '', 'foo bar', 'foo bar', '', 'bar', 'foo bar',\n",
    "        'foo bar', 'bar', 'foo bar', 'bar foo', 'bar foo', '', 'bar foo',\n",
    "        'bar foo', 'bar', 'foo bar', '', 'foo', 'foo', 'bar', '', 'bar foo',\n",
    "        'bar foo', 'foo', 'bar foo', '', 'bar foo', 'foo', 'bar foo', '',\n",
    "        '', 'bar foo', 'foo bar', 'bar', '', '', 'foo', 'foo bar', 'foo',\n",
    "        'foo', 'foo', 'foo bar', 'foo', 'bar foo', '', 'bar', 'bar',\n",
    "        'foo bar', 'foo bar', 'bar', 'bar foo', '', 'bar foo', 'bar', 'bar',\n",
    "        '', '', 'foo bar', 'foo bar', 'bar foo', 'bar', 'bar foo',\n",
    "        'foo bar', 'foo', '', 'bar', 'bar', 'foo', 'bar', 'bar', 'foo',\n",
    "        'foo bar', '', 'foo bar', 'foo', 'bar foo', '', 'bar foo', 'bar',\n",
    "        '', 'bar foo', 'bar foo', 'bar foo', 'bar', '', 'foo bar', 'bar',\n",
    "        '', 'bar foo', 'foo', 'foo', 'bar', 'foo bar', 'foo bar', '', 'foo',\n",
    "        'bar foo', 'foo bar', 'foo bar', 'foo', 'bar foo', '', '', '', '',\n",
    "        'foo', '', 'foo', 'foo bar', 'foo', 'bar', 'bar', 'foo bar',\n",
    "        'bar foo', 'bar', 'foo', 'foo bar', 'bar foo', 'bar', 'bar foo', '',\n",
    "        '', '', 'bar', 'bar foo', 'bar', 'bar foo', 'foo', 'foo', 'bar',\n",
    "        'bar foo', 'foo', 'bar foo', 'foo', 'bar foo', 'bar', 'bar',\n",
    "        'foo bar', 'foo', '', 'foo bar', '', 'foo', 'bar', '', 'bar',\n",
    "        'foo bar', 'foo bar', 'bar', 'bar foo', 'foo', 'bar foo', 'foo bar',\n",
    "        'foo', 'bar foo', '', 'bar foo', 'bar', '', 'bar', '', 'foo bar',\n",
    "        'foo', '', 'foo bar', 'foo bar', 'foo', 'foo', '', 'foo bar',\n",
    "        'foo bar', 'foo', 'bar', 'foo', 'bar', 'bar', 'bar foo', 'bar',\n",
    "        'foo', '', 'foo bar', 'foo', '', 'bar', 'bar foo', 'foo', 'foo bar',\n",
    "        'bar', 'bar foo', 'bar foo', 'bar', 'foo', 'bar', 'bar', 'foo', '',\n",
    "        'bar', 'foo', 'bar', 'bar', 'foo bar', 'foo bar', 'bar', 'foo bar',\n",
    "        'bar', 'foo', 'bar', 'bar', 'foo', '', 'bar', 'bar', 'bar foo',\n",
    "        'foo', '', 'bar', 'bar', 'foo', '', '', 'bar foo', 'foo', 'bar',\n",
    "        'foo bar', 'foo', 'bar foo', '', 'bar foo', 'bar', 'bar', '', '',\n",
    "        'bar', 'foo bar', '', 'foo', 'bar foo', 'bar foo', 'foo', '',\n",
    "        'foo bar', 'foo bar', 'foo', 'bar', '', '', 'bar', 'bar', 'bar',\n",
    "        'bar foo', 'foo', 'foo bar', 'bar foo', 'foo bar', '', '', '',\n",
    "        'bar foo', '', 'foo bar', 'foo bar', 'bar', '', 'foo bar', '',\n",
    "        'foo', 'bar foo', 'foo', 'bar', 'foo', '', 'foo bar', 'bar foo',\n",
    "        'bar', 'bar', 'bar foo', 'foo bar', 'bar', 'foo bar', 'bar',\n",
    "        'foo bar', '', 'bar', 'foo', 'bar', 'foo bar', 'bar foo', 'bar foo',\n",
    "        'foo bar', '', 'foo', 'bar', 'bar', 'foo bar', 'bar', 'foo bar',\n",
    "        'foo bar', 'foo bar', 'bar foo', 'bar', 'bar foo', 'foo bar',\n",
    "        'foo bar', 'foo bar', 'bar', 'bar foo', 'bar', 'foo', 'foo', 'foo',\n",
    "        'foo', 'foo bar', 'bar foo', '', '', 'foo', 'bar foo', 'bar', 'bar',\n",
    "        'foo', 'foo', 'bar', '', '', 'foo bar', '', 'bar', 'foo', 'bar',\n",
    "        'bar', 'foo bar', '', '', 'bar', 'foo bar', 'bar', 'bar foo',\n",
    "        'bar foo', '', 'bar', '', 'bar foo', 'bar foo', 'bar', 'foo bar',\n",
    "        '', 'bar', 'bar foo', '', 'foo', '', 'foo bar', 'bar', 'bar',\n",
    "        'foo bar', '', '', 'bar foo', '', 'foo', 'foo bar', 'bar foo',\n",
    "        'bar', 'foo bar', 'bar', 'foo bar', 'bar', 'bar', 'bar foo', '', '',\n",
    "        'foo bar', 'bar foo', '', 'bar', 'foo bar', '', 'foo', '', 'bar',\n",
    "        'bar foo', 'foo bar', 'bar', 'bar', 'foo', 'bar', 'foo', 'bar', '',\n",
    "        'foo bar', 'bar foo', 'foo', 'bar foo', 'foo bar', 'foo', 'bar',\n",
    "        'foo', 'bar foo', 'foo bar', 'bar', '', 'bar', 'bar', '', 'bar',\n",
    "        'bar', 'bar foo', '', 'foo', 'bar', 'bar', 'bar foo', 'bar',\n",
    "        'bar foo', 'bar foo', 'bar', 'bar', 'bar', '', 'foo bar', 'foo bar',\n",
    "        'bar foo', '', '', 'foo', 'foo', 'foo bar', '', 'bar', 'bar foo',\n",
    "        'foo', 'bar', 'bar foo', '', 'foo', 'foo', 'foo bar', 'foo', '',\n",
    "        'foo bar', 'bar foo', 'foo', 'foo bar', '', '', 'bar', 'foo bar',\n",
    "        'bar foo', '', '', 'bar', 'foo bar', 'bar foo', 'foo', 'bar', '',\n",
    "        'foo', 'foo bar', 'bar', 'bar foo', '', '', 'bar foo', '', 'foo',\n",
    "        'foo bar', '', 'bar', 'foo', 'bar', 'foo bar', 'foo bar', '', '',\n",
    "        '', 'foo', 'bar', 'bar foo', 'foo', 'bar foo', 'bar foo', 'bar foo',\n",
    "        '', 'bar foo', 'bar', '', 'bar', 'foo bar', 'bar', 'bar foo',\n",
    "        'foo bar', 'bar foo', 'foo', 'bar foo', 'foo bar', 'bar foo',\n",
    "        'foo bar', 'foo', 'bar foo', 'foo bar', 'foo bar', 'foo bar',\n",
    "        'foo bar', 'bar foo', '', 'bar', 'foo bar', '', 'bar foo', 'foo',\n",
    "        'foo bar', '', '', 'foo', '', '', 'bar', 'bar foo', 'foo',\n",
    "        'foo bar', 'bar', 'foo bar', 'bar', '', 'foo', 'bar', '', 'bar',\n",
    "        'bar foo', 'foo', 'bar foo', 'foo bar', 'bar foo', '', 'foo bar',\n",
    "        'foo bar', 'foo bar', 'foo', 'foo bar', 'bar foo', 'foo bar', 'foo',\n",
    "        'bar', 'foo', 'foo', 'bar foo', 'bar', '', 'foo bar', 'foo bar',\n",
    "        'bar foo', '', 'bar', 'foo', 'foo bar', 'bar', 'bar foo', 'foo',\n",
    "        'bar foo', 'foo', 'foo', 'foo bar', 'bar foo', 'bar', 'bar',\n",
    "        'foo bar', '', 'foo', 'foo', 'bar', 'bar', '', 'bar foo', '',\n",
    "        'bar foo', 'bar foo', 'foo bar', 'foo bar', 'bar foo', 'bar foo',\n",
    "        'bar foo', '', 'foo bar', '', 'foo bar', 'foo', 'foo', 'bar',\n",
    "        'foo bar', 'foo bar', 'bar foo', 'bar', 'foo', 'foo', '', 'foo', '',\n",
    "        '', '', 'foo', 'foo bar', 'bar foo', 'bar foo', '', 'foo bar',\n",
    "        'bar', 'foo', 'foo', 'bar foo', '', 'foo bar', 'foo bar', 'foo',\n",
    "        'foo bar', 'foo bar', 'foo', 'foo bar', 'foo bar', 'bar foo', 'foo',\n",
    "        '', '', 'bar foo', 'foo', 'bar foo', '', 'foo', 'foo', 'bar foo',\n",
    "        'foo', 'foo', 'foo bar', '', 'bar', 'bar foo', 'foo bar', 'foo',\n",
    "        'bar', 'foo', 'bar', 'bar foo', 'bar foo', 'foo', '', 'foo', 'bar',\n",
    "        'foo bar', '', 'bar', '', '', 'bar foo', '', 'bar foo', 'foo bar',\n",
    "        '', 'foo bar', 'foo', '', '', 'bar', 'foo', '', 'foo', 'foo',\n",
    "        'bar foo', '', 'bar', 'foo bar', '', 'bar foo', 'foo', '', 'bar',\n",
    "        'foo bar', 'bar foo', '', '', '', '', 'foo', '', 'foo', 'bar foo',\n",
    "        'bar foo', '', 'foo', 'foo', 'foo', 'bar foo', 'bar foo', 'foo bar',\n",
    "        'bar foo', 'bar', '', '', 'bar foo', 'foo', '', 'bar foo',\n",
    "        'foo bar', 'bar', 'bar', 'foo bar', 'bar foo', '', 'bar foo',\n",
    "        'bar foo', 'bar', 'foo bar', 'bar', 'foo', 'bar', 'bar foo',\n",
    "        'foo bar', 'foo bar', '', 'foo', 'foo', 'foo', 'bar foo', '', 'foo',\n",
    "        '', 'bar', 'foo', 'bar', '', 'foo bar', 'foo bar']),\n",
    " 'with_missing': np.array([ 4.43324013,  4.31022893,  2.46982849,  2.85298126,  1.8264749 ,\n",
    "         2.76431471,  3.02431714,  2.59604007,  2.49641543,  4.03208018,\n",
    "         3.90748295,  3.39912205,  1.7224894 ,  3.2009309 ,  2.61576843,\n",
    "         4.31572746,  1.19770925,  2.44117631,  2.41334969,  2.53342597,\n",
    "         3.95757875,  0.64764159,  2.79194782,  3.85072067,  3.98011751,\n",
    "         2.45092214,  2.32022185,  3.34621357,  3.15103669,  3.06940312,\n",
    "        -0.8013782 ,  1.8728278 ,  2.13677694,  3.64300986,  1.5592403 ,\n",
    "         1.95337244,  2.84561599,  1.71240482,  3.64285391,  2.60859632,\n",
    "         4.83290593,  2.37028128,  2.4237168 ,  3.80443829,  2.34466307,\n",
    "         4.51156832,  3.48291494,  2.69612006,  2.839202  ,  5.85070774,\n",
    "         3.48865436,  3.15047702,  2.86742324,  4.03716203,  1.48167369,\n",
    "         2.81881198,  0.16634254,  2.46659237,  3.84792681,  4.38223084,\n",
    "         3.60597024,  3.63996859,  1.28409851,  2.32162619,  4.47440502,\n",
    "         2.47613999,  2.8640176 ,  2.19443801,  2.06624851,  2.52973077,\n",
    "         2.33894403,  3.9575018 ,  3.2153696 ,  4.01888383,  1.6127625 ,\n",
    "         4.22545315,  3.02145622,  4.83934838,  2.94722386,  1.50659819,\n",
    "         4.28422063,  2.86351176,  0.84788492,  3.14519468,  3.23122813,\n",
    "         4.07611274, -0.58749383,  4.14886948,  2.52540325,  4.92624791,\n",
    "         1.281719  ,  3.5453389 ,  3.41171883,  2.57788527,  2.22619409,\n",
    "         2.62625497,  2.65327971,  1.74656199,  3.99299217,  1.90779965,\n",
    "         4.02319098,  1.21106543,  2.82281022,  2.83769347,  2.98420563,\n",
    "         3.95629588,  2.79765278,  2.54207059,  2.80067242,  3.98100686,\n",
    "         1.79439305,  1.9793315 ,  1.04330175,  5.0272487 ,  1.84025527,\n",
    "         1.11456834,  2.19110086,  2.38754475,  4.465456  ,  2.53510077,\n",
    "         3.29176928,  1.2092202 ,  3.20798349,  1.35120376,  2.68640495,\n",
    "         2.24012902,  3.0582715 ,  3.63440871,  1.66410405,  1.86358157,\n",
    "         3.19358901,  3.68728571,  4.4173752 ,  2.52810827,  1.50852133,\n",
    "         3.18662572,  2.20535628,  2.0755798 ,  4.40817707,  3.61871681,\n",
    "         2.27192214,  2.98128846,  2.91432301,  3.16509562,  2.59406039,\n",
    "         1.09993747,  2.02364982,  2.46800341,  3.02811177,  1.91770564,\n",
    "         3.18628494,  2.61872854,  2.2405249 ,  1.72972388,  3.44649142,\n",
    "         3.42424339,  3.80022088,  3.54928823,  3.62413445,  3.38207862,\n",
    "         1.24159032,  2.72088112,  1.56761754,  4.36921208,  2.98598628,\n",
    "         4.26975585,  1.96967771,  2.90813078,  3.26337425,  2.66909785,\n",
    "         3.1877894 ,  4.48216346,  1.96808658,  0.36207751,  4.60062414,\n",
    "         2.20833776,  4.1224997 ,  3.6519133 ,  2.7833132 ,  3.38034479,\n",
    "         1.85946574,  3.08589715,  3.5940402 ,  2.22566727,  2.6197089 ,\n",
    "         3.31259352,  4.32327371,  2.99684516,  2.56934016,  2.85358423,\n",
    "         4.16017595,  2.35848766,  2.6997676 ,  2.36773922,  2.79568347,\n",
    "         3.2136956 ,  4.0338782 ,  0.87542472,  3.55548892,  2.10841012,\n",
    "         3.59307001,  4.23963499,  3.99559501,  3.15795878,  3.83449893,\n",
    "         2.05661606,  2.93216991,  4.5194711 ,  2.21074447,  3.19726917,\n",
    "         2.63082463,  2.80489153,  3.4060712 ,  2.73048109,  2.99659085,\n",
    "         2.49541673,  1.17497167,  3.19309413,  3.43947004,  4.38217346,\n",
    "         2.86813454,  1.60529147,  2.24599181,  3.54039235,  2.91547049,\n",
    "         3.27929088,  2.64691986,  4.86980891,  4.72798801,  2.06840437,\n",
    "         3.90903099,  3.30406239,  3.01355953,  2.14031201,  4.62153063,\n",
    "         4.16525834,  1.54289995,  4.38349131,  2.96162173,  2.40220943,\n",
    "         2.02342224,  3.94482031,  3.39989552,  2.12585368,  2.18890221,\n",
    "         2.6124769 ,  2.48749395,  3.52778031,  3.37873699,  2.3643501 ,\n",
    "         1.34249058,  1.66600073,  3.87005551,  2.00518953,  3.1111352 ,\n",
    "         1.23684247,  3.39705984,  4.91952641,  1.67193277,  3.54969727,\n",
    "         3.27246193,  3.91788395,  2.86875968,  3.27282494,  3.18197413,\n",
    "         3.57284282,  2.16088729,  3.19244876,  3.8103256 ,  4.16799059,\n",
    "         3.49572892,  1.86804347,  1.44642182,  3.40862556,  1.98835625,\n",
    "         3.84433461,  2.71221564,  4.01161594,  5.12581662,  2.93767732,\n",
    "         2.88399961,  4.01349295,  4.06390503,  4.70160817,  3.19656962,\n",
    "         2.36713304,  3.00771366,  2.37637081,  3.2398097 ,  2.13176031,\n",
    "         3.90200646,  3.69749015,  3.1390333 ,  3.02557655,  2.80355708,\n",
    "         4.3122552 ,  4.18741748,  3.14571685,  2.14335012,  1.82315168,\n",
    "         3.28762071,  2.36193806,  3.47593387,  2.61330446,  3.77375436,\n",
    "         3.20741524,  3.99805306,  0.76609475,  2.77294975,  5.21678813,\n",
    "         0.49683899,  3.84265222,  3.41800374,  4.28570105,  2.8264082 ,\n",
    "         4.19035444,  4.2169058 ,  3.19678009,  1.83598812,  3.91300453,\n",
    "         2.026563  ,  2.86262343,  3.94690824,  4.09206771,  2.65204103,\n",
    "         3.62698294,  1.45746339,  2.5276191 ,  4.4689189 ,  4.34338805,\n",
    "         3.66921384,  2.85816551,  3.84726032,  2.97677181,  3.16822861,\n",
    "         3.08838291,  2.22447969,  3.72592481,  1.93005274,  3.77584485,\n",
    "         2.03939753,  2.27792428,  0.15915264,  2.84391655,  3.18918897,\n",
    "         3.82707251,  4.31087601,  1.50978657,  3.98482749,  2.53596575,\n",
    "         2.9279399 ,  2.89473164,  2.55048533,  2.11008894,  3.27921672,\n",
    "         3.81114504,  1.74022829,  2.98054124,  2.37098775,  4.09592178,\n",
    "         3.49357955,  4.50870511,  1.451235  ,  4.0341966 ,  3.16266541,\n",
    "         2.27482655,  3.48920349,  4.56603327,  4.60461412,  3.8222182 ,\n",
    "         2.00839531,  3.30986345,  3.34242258,  4.24201662,  2.83512482,\n",
    "         3.01557178,  2.34649131,  3.66808732,  2.78458951,  3.18597859,\n",
    "         2.53044999,  3.04723961,  1.919587  ,  3.16088917,  2.39742478,\n",
    "         3.69932855,  4.84745342,  3.84570124,  1.88007749,  2.64070328,\n",
    "         1.39030492,  3.01357006,  1.22577645,  1.79862269,  4.09625679,\n",
    "         3.86103685,  1.47963288,  2.55255984,  3.46348725,  3.39249326,\n",
    "         1.3728328 ,  3.26000968,  2.39214737,  4.1986679 ,  2.79982023,\n",
    "         3.35928747,  2.89087939,  3.66644446,  2.23193058,  3.66825329,\n",
    "         4.20896282,  4.83556065,  4.58378961,  2.46387817,  3.57714972,\n",
    "         3.80749724,  3.59004486,  3.75347847,  3.42394361,  2.45474459,\n",
    "         3.20711055,  3.8467867 ,  3.46160539,  1.90364649,  2.12238172,\n",
    "         2.59075838,  2.7951865 ,  3.15727323,  2.0229055 ,  3.79495583,\n",
    "         1.14243568,  3.52870169,  3.50653343,  3.64409865, -0.06698763,\n",
    "         1.65072452,  2.64730398,  3.16828204,  2.9186086 ,  2.1862534 ,\n",
    "         4.28304399,  3.3240295 ,  2.60524508,  2.01242847,  3.06821329,\n",
    "         4.25870044,  3.03517983,  2.96795257,  3.06628239,  3.12061587,\n",
    "         2.76681343,  2.02714499,  4.52901431,  3.6374926 ,  5.1171334 ,\n",
    "         3.49755594,  2.76870806,  2.05416332,  2.67000423,  1.84705705,\n",
    "         5.09784068,  2.98296829,  3.78605661,  3.96196866,  4.08091513,\n",
    "         5.54786978,  2.9045881 ,  2.81813794,  3.16448029,  3.74854153,\n",
    "         3.17887711,  2.73114873,  2.2840626 ,  2.22565052,  3.26947268,\n",
    "         2.00112175,  1.1859674 ,  3.11523953,  4.16262992,  3.66181095,\n",
    "         2.98211216,  3.40300183,  1.3528843 ,  2.31035984,  4.67806871,\n",
    "         4.18411875,  2.28842733,  4.75431226,  2.28210911,  2.17680796,\n",
    "         4.53591757,  4.30774005,  2.4114405 ,  2.95091118,  2.65830002,\n",
    "         3.50413856,  4.56739192,  3.53215454,  2.88609559,  1.65369999,\n",
    "         3.77796313,  1.77705285,  2.81286693,  3.95823564,  3.81122546,\n",
    "         3.83871328,  3.69892124,  3.14585946,  3.30215462,  3.09207026,\n",
    "         3.48257743,  3.53523644,  3.13332849,  3.51381412,  2.31517897,\n",
    "         2.69304332,  3.23049856,  4.92351298,  3.52899084,  2.86526405,\n",
    "         2.77977614,  3.74416551,  2.90273868,  3.63738099,  3.51160406,\n",
    "         1.75392203,  3.47278502,  3.46562381,  3.79666787,  2.16291661,\n",
    "         4.0941101 ,  2.83320251,  4.98827433,  4.75243295,  2.39193981,\n",
    "         1.63078141,  3.69724244,  2.18003097,  4.14750752,  3.52986188,\n",
    "         1.54639101,  3.78206424,  3.13725092,  3.41211162,  4.5240466 ,\n",
    "         2.87007784,  3.75802173,  2.40161133,  2.42619507,  2.78185249,\n",
    "         2.3796186 ,  3.5678615 ,  1.67656373,  3.82954714,  2.23847141,\n",
    "         3.37770456,  1.81256674,  3.47773237,  4.0840075 ,  3.00977117,\n",
    "         4.54868224,  2.86009265,  3.41136775,  2.07453303,  3.4384309 ,\n",
    "         2.43186071,  2.27960869,  4.58010361,  1.72977914,  2.88523154,\n",
    "         3.71279026,  3.23407091,  1.15267301,  3.60390927,  1.62100104,\n",
    "         2.55674805,  2.75575735,  0.07997068,  2.89409944,  3.6658774 ,\n",
    "         2.82991557,  3.18492981,  5.26178685,  4.12544023,  3.05938582,\n",
    "         5.58700883,  2.36362898,  3.51967711,  1.4233761 ,  3.39832801,\n",
    "         2.88858969,  3.06079327,  3.37048139,  3.28000391,  1.92737276,\n",
    "         2.61665486,  3.00606071,  1.57082439,  3.80256945,  4.88288231,\n",
    "         3.30354353,  2.47228683,  0.69807554,  1.47909174,  3.33922886,\n",
    "         4.26243654,  2.77991563,  1.51275873,  2.12897476,  2.42777125,\n",
    "         1.06745264,  2.68943376,  3.89238676,  1.31821808,  2.44035995,\n",
    "         0.97569572,  2.18085066,  2.85195597,  4.02120621,  3.45570863,\n",
    "         1.8165523 ,  3.09398131,  2.66606659,  2.98712987,  2.80050517,\n",
    "         3.31337207,  3.18111264,  3.86643348,  3.11659189,  4.68993071,\n",
    "         3.51922618,  4.47831033,  3.72025002,  2.32721426,  4.2429179 ,\n",
    "         4.18417713,  0.12794139,  2.45155465,  4.44275911,  4.05438804,\n",
    "         1.53958249,  3.85876182,  4.7272349 ,  3.60669842,  1.52414382,\n",
    "         2.21618467,  3.40050437,  2.78446504,  4.27766268,  3.73124874,\n",
    "         4.96317768,  4.02061691,  3.55253735,  2.68566568,  4.40446176,\n",
    "         3.00549834,  1.97409993,  3.04817082,  2.90814442,  2.00645981,\n",
    "         3.82783848,  2.85957177,  3.39558862,  2.21909139,  2.8353288 ,\n",
    "         3.77432616,  2.7531717 ,  2.63465843,  5.1843137 ,  3.31619493,\n",
    "         3.73330653,  3.74623793,  3.09696364,  3.52762463,  2.0269882 ,\n",
    "         4.44820855,  4.24694976,  3.19021457,  3.00635588,  1.76037693,\n",
    "         4.02931097,  2.7449632 ,  3.60976691,  1.55932601,  3.63145462,\n",
    "         2.82474895,  3.01190787,  1.58218823,  3.6790643 ,  2.38747465,\n",
    "         2.95298875,  3.804181  ,  2.91433115,  4.98624296,  2.10119693,\n",
    "         3.24048575,  3.41332316,  1.98102722,  3.08740901,  2.80511824,\n",
    "         3.22581443,  3.45364798,  4.39825083,  3.28817383,  3.54706913,\n",
    "         4.49931597,  1.41573626,  2.73880318,  3.03817186,  1.44443966,\n",
    "         3.07668047,  0.64419409,  3.70553772,  1.52171456,  1.74881506,\n",
    "         3.52434435,  1.33242665,  5.05378027,  3.11660277,  2.90290692,\n",
    "         4.20319285,  2.92029116,  3.24063119,  2.84409982,  2.4981829 ,\n",
    "         3.37118751,  3.52502805,  1.72686175,  2.11882703,  2.34058495,\n",
    "         2.97041178,  3.34566062,  5.15608564,  3.27504015,  2.82565632,\n",
    "         2.28511923,  3.98528257,  2.98809753,  3.34175457,  2.99932084,\n",
    "         3.55057734,  3.47870001,  3.37384271,  3.00383309,  3.09342879,\n",
    "         3.38231956,  1.07580382,  2.96592737,  3.98078947,  2.51968549,\n",
    "         0.909881  ,  1.61496494,  3.07519754,  2.27924043,  3.13392494,\n",
    "         2.98153783,  2.5308459 ,  2.08500218,  3.19548695,  3.0689317 ,\n",
    "         2.8761105 ,  3.87241444,  3.20133444,  2.40396846,  2.64140605,\n",
    "         2.93806905,  3.12319077,  4.33641517,  4.68807724,  2.53105469,\n",
    "         1.98059287,  2.45711849,  3.42012626,  3.83278635,  2.59188514,\n",
    "         2.08582342,  3.38236117,  3.62359292,  4.03464371,  2.36228279,\n",
    "         3.32886248,  1.36830043,  2.89430294,  3.71851451,  4.48372069,\n",
    "         3.30216295,  2.75824986,  1.59403907,  3.2283711 ,  3.63175892,\n",
    "         4.01366576,  3.24321171,  2.04452863,  3.55350384,  4.68987953,\n",
    "         2.73760143,  2.19755393,  2.67032319,  2.98682136,  2.47651847,\n",
    "         3.70633284,  3.43073018,  2.66567469,  2.98017476,  1.46093415,\n",
    "         3.32125428,  1.60619336,  2.72266844,  2.53865554,  3.466579  ,\n",
    "         2.55585794,  2.95304205,  4.49519203,  1.43056796,  3.11310163,\n",
    "         3.86420927,  2.3722498 ,  3.02092246,  1.63867114,  1.39122086,\n",
    "         3.72210902,  2.6728564 ,  1.68731515,  2.52662105,  3.6044344 ,\n",
    "         3.55123172,  4.04773417,  2.49600315,  2.37256342,  3.3308814 ,\n",
    "         4.00009556,  2.62544144,  3.95819114,  2.66653018,  5.03745194,\n",
    "         1.2803261 ,  4.53483488,  1.5025198 ,  4.22317336,  3.08580064,\n",
    "         2.54665709,  2.09122976,  2.07187786,  3.68175596,  3.46030921,\n",
    "         2.81155547,  2.97341576,  2.1287188 ,  2.86743255,  3.88584111,\n",
    "         2.092408  ,  3.52113751,  2.53855314,  1.47243073,  4.06167883,\n",
    "         1.62605284,  2.53517227,  4.06925344,  2.21278921,  3.94255342,\n",
    "         2.55149843,  2.90641082,  4.88454748,  4.13579066,  3.08618543,\n",
    "         2.34600945,  3.02688627,  4.64960973,  5.17205091,  2.38016797,\n",
    "         4.44424506,  1.42217221,  3.58820641,  3.37436642,  4.04109692,\n",
    "         2.16759117,  3.14601715,  1.13824025,  2.36993478,  2.98299642,\n",
    "         3.19451308,  5.79197117,  2.49060501,  3.02085102,  1.84799754,\n",
    "         1.1136034 ,  3.89835491,  3.90545284,  1.62455946,  1.31020911,\n",
    "         2.36411839,  4.66702579,  1.51185865,  3.68179686,  3.13142663,\n",
    "         2.69818255,  2.3953984 ,  3.24498205,  2.68920493,  2.6006796 ,\n",
    "         2.79246025,  4.11162249,  3.21284144,  3.06895192,  4.38257879,\n",
    "         2.73185155,  4.43283757,  2.3241877 ,  3.53114891,  2.67689671,\n",
    "         2.93150372,  2.35676076,  2.70461329,  2.29940912,  3.4028518 ,\n",
    "         2.17523185,  3.53655772,  5.34706587,  1.44495871,  0.62787862,\n",
    "         2.01180447,  3.7432968 ,  2.28884   ,  3.57748109,  3.94922125,\n",
    "         2.74892188,  3.88476825,  0.73049823,  3.83453273,  3.79930337,\n",
    "         2.90056714,  4.00353656,  4.44096288,  4.93282058,  4.9412493 ,\n",
    "         3.05552553,  2.8587828 ,  2.92189871,  0.32550971,  3.69815678,\n",
    "         5.34935569,  3.56583355,  1.57852116,  2.0053083 ,  2.06157426,\n",
    "         4.65705198,  3.50239382,  3.63812863,  3.35352971,  1.88852127,\n",
    "         2.05608929,  3.68073848,  3.08621155,  1.14190158,  3.786027  ,\n",
    "         4.41672969,  3.00639188,  2.68403549,  4.36579236,  2.80426824,\n",
    "         2.38617807,  1.46149643,  2.5088012 ,  4.86700063,  4.16380471,\n",
    "         3.11936482,  2.99412604,  3.98963932,  3.1023014 ,  1.73609526,\n",
    "         3.14429203,  3.17551025,  3.01290446,  3.25744994,  2.30709921,\n",
    "         3.15507208,  1.39672136,  2.07936858,  2.7561449 ,  3.60253756,\n",
    "         2.70266861,  3.44019839,  4.02267746,  5.42023327,  2.70685622,\n",
    "         2.35626862,  3.03858914,  3.19842601,  3.54612338,  3.81255351,\n",
    "         2.71760723,  3.72693458,  3.08196618,  4.22737164,  2.72118737,\n",
    "         3.1402556 ,  3.27025617,  4.13252526,  2.99589649,  1.96339612])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "sample_df['label'] = np.array(['b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'a',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'b',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b',\n",
    " 'a',\n",
    " 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 4 columns):\n",
      "numeric         1000 non-null float64\n",
      "text            1000 non-null object\n",
      "with_missing    822 non-null float64\n",
      "label           1000 non-null object\n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 31.3+ KB\n"
     ]
    }
   ],
   "source": [
    "nan_rows = random.sample(range(1000),178)\n",
    "sample_df.loc[nan_rows, 'with_missing'] = np.NaN\n",
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - numeric, no nans:  0.62\n"
     ]
    }
   ],
   "source": [
    "# Import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import other necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Split and select numeric data only, no nans \n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric']],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=22)\n",
    "\n",
    "# Instantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - numeric, no nans: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - all numeric, incl nans:  0.632\n"
     ]
    }
   ],
   "source": [
    "# Import the Imputer object\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Create training and test sets using only numeric data\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing']],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=456)\n",
    "\n",
    "# Insantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('imp', Imputer()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - all numeric, incl nans: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - just text data:  0.808\n"
     ]
    }
   ],
   "source": [
    "# Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Split out only the text data\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df['text'],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=456)\n",
    "\n",
    "# Instantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('vec', CountVectorizer()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - just text data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 4 columns):\n",
      "numeric         1000 non-null float64\n",
      "text            1000 non-null object\n",
      "with_missing    822 non-null float64\n",
      "label           1000 non-null object\n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 31.3+ KB\n"
     ]
    }
   ],
   "source": [
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Data\n",
      "0           \n",
      "1        foo\n",
      "2    foo bar\n",
      "3           \n",
      "4    foo bar\n",
      "Name: text, dtype: object\n",
      "\n",
      "Numeric Data\n",
      "     numeric  with_missing\n",
      "0 -10.856306           NaN\n",
      "1   9.973454           NaN\n",
      "2   2.829785           NaN\n",
      "3 -15.062947           NaN\n",
      "4  -5.786003           NaN\n"
     ]
    }
   ],
   "source": [
    "# Import FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Obtain the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)\n",
    "\n",
    "# Obtain the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[['numeric', 'with_missing']], validate=False)\n",
    "\n",
    "# Fit and transform the text data: just_text_data\n",
    "just_text_data = get_text_data.fit_transform(sample_df)\n",
    "\n",
    "# Fit and transform the numeric data: just_numeric_data\n",
    "just_numeric_data = get_numeric_data.fit_transform(sample_df)\n",
    "\n",
    "# Print head to check results\n",
    "print('Text Data')\n",
    "print(just_text_data.head())\n",
    "print('\\nNumeric Data')\n",
    "print(just_numeric_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - all data:  0.928\n"
     ]
    }
   ],
   "source": [
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Split using ALL data in sample_df\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing', 'text']],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=22)\n",
    "\n",
    "# Create a FeatureUnion with nested pipeline: process_and_join_features\n",
    "process_and_join_features = FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )\n",
    "\n",
    "# Instantiate nested pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', process_and_join_features),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "\n",
    "# Fit pl to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - all data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['Function',\n",
    " 'Use',\n",
    " 'Sharing',\n",
    " 'Reporting',\n",
    " 'Student_Type',\n",
    " 'Position_Type',\n",
    " 'Object_Type',\n",
    " 'Pre_K',\n",
    " 'Operating_Status']\n",
    "\n",
    "NUMERIC_COLUMNS = ['FTE', 'Total']\n",
    "\n",
    "df = pd.read_csv('TrainingData.csv', index_col=0)\n",
    "\n",
    "# Define the lambda function: categorize_label\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "\n",
    "# Convert df[LABELS] to a categorical type\n",
    "df[LABELS] = df[LABELS].apply(categorize_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code prep\n",
    "def multilabel_sample(y, size=1000, min_count=5, seed=None):\n",
    "    \"\"\" Takes a matrix of binary labels `y` and returns\n",
    "        the indices for a sample of size `size` if\n",
    "        `size` > 1 or `size` * len(y) if size =< 1.\n",
    "        The sample is guaranteed to have > `min_count` of\n",
    "        each label.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if (np.unique(y).astype(int) != np.array([0, 1])).all():\n",
    "            raise ValueError()\n",
    "    except (TypeError, ValueError):\n",
    "        raise ValueError('multilabel_sample only works with binary indicator matrices')\n",
    "\n",
    "    if (y.sum(axis=0) < min_count).any():\n",
    "        raise ValueError('Some classes do not have enough examples. Change min_count if necessary.')\n",
    "\n",
    "    if size <= 1:\n",
    "        size = np.floor(y.shape[0] * size)\n",
    "\n",
    "    if y.shape[1] * min_count > size:\n",
    "        msg = \"Size less than number of columns * min_count, returning {} items instead of {}.\"\n",
    "        warn(msg.format(y.shape[1] * min_count, size))\n",
    "        size = y.shape[1] * min_count\n",
    "\n",
    "    rng = np.random.RandomState(seed if seed is not None else np.random.randint(1))\n",
    "\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        choices = y.index\n",
    "        y = y.values\n",
    "    else:\n",
    "        choices = np.arange(y.shape[0])\n",
    "\n",
    "    sample_idxs = np.array([], dtype=choices.dtype)\n",
    "\n",
    "    # first, guarantee > min_count of each label\n",
    "    for j in range(y.shape[1]):\n",
    "        label_choices = choices[y[:, j] == 1]\n",
    "        label_idxs_sampled = rng.choice(label_choices, size=min_count, replace=False)\n",
    "        sample_idxs = np.concatenate([label_idxs_sampled, sample_idxs])\n",
    "\n",
    "    sample_idxs = np.unique(sample_idxs)\n",
    "\n",
    "    # now that we have at least min_count of each, we can just random sample\n",
    "    sample_count = int(size - sample_idxs.shape[0])\n",
    "\n",
    "    # get sample_count indices from remaining choices\n",
    "    remaining_choices = np.setdiff1d(choices, sample_idxs)\n",
    "    remaining_sampled = rng.choice(remaining_choices,\n",
    "                                   size=sample_count,\n",
    "                                   replace=False)\n",
    "\n",
    "    return np.concatenate([sample_idxs, remaining_sampled])\n",
    "\n",
    "\n",
    "def multilabel_sample_dataframe(df, labels, size, min_count=5, seed=None):\n",
    "    \"\"\" Takes a dataframe `df` and returns a sample of size `size` where all\n",
    "        classes in the binary matrix `labels` are represented at\n",
    "        least `min_count` times.\n",
    "    \"\"\"\n",
    "    idxs = multilabel_sample(labels, size=size, min_count=min_count, seed=seed)\n",
    "    return df.loc[idxs]\n",
    "\n",
    "\n",
    "def multilabel_train_test_split(X, Y, size, min_count=5, seed=None):\n",
    "    \"\"\" Takes a features matrix `X` and a label matrix `Y` and\n",
    "        returns (X_train, X_test, Y_train, Y_test) where all\n",
    "        classes in Y are represented at least `min_count` times.\n",
    "    \"\"\"\n",
    "    index = Y.index if isinstance(Y, pd.DataFrame) else np.arange(Y.shape[0])\n",
    "\n",
    "    test_set_idxs = multilabel_sample(Y, size=size, min_count=min_count, seed=seed)\n",
    "    train_set_idxs = np.setdiff1d(index, test_set_idxs)\n",
    "\n",
    "    test_set_mask = index.isin(test_set_idxs)\n",
    "    train_set_mask = ~test_set_mask\n",
    "\n",
    "    return (X[train_set_mask], X[test_set_mask], Y[train_set_mask], Y[test_set_mask])\n",
    "\n",
    "# Define combine_text_columns()\n",
    "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
    "    \n",
    "    # Drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data = data_frame.drop(to_drop, axis=1)\n",
    "    \n",
    "    # Replace nans with blanks\n",
    "    text_data.fillna(\"\", inplace=True)\n",
    "    \n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Get the dummy encoding of the labels\n",
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Get the columns that are features in the original df\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],\n",
    "                                                               dummy_labels,\n",
    "                                                               0.2, \n",
    "                                                               seed=123)\n",
    "\n",
    "# Preprocess the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "\n",
    "# Preprocess the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.3695459371681969\n"
     ]
    }
   ],
   "source": [
    "# Complete the pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400277 entries, 0 to 400276\n",
      "Data columns (total 26 columns):\n",
      "Unnamed: 0                400277 non-null int64\n",
      "Function                  400277 non-null category\n",
      "Use                       400277 non-null category\n",
      "Sharing                   400277 non-null category\n",
      "Reporting                 400277 non-null category\n",
      "Student_Type              400277 non-null category\n",
      "Position_Type             400277 non-null category\n",
      "Object_Type               400277 non-null category\n",
      "Pre_K                     400277 non-null category\n",
      "Operating_Status          400277 non-null category\n",
      "Object_Description        375493 non-null object\n",
      "Text_2                    88217 non-null object\n",
      "SubFund_Description       306855 non-null object\n",
      "Job_Title_Description     292743 non-null object\n",
      "Text_3                    109152 non-null object\n",
      "Text_4                    53746 non-null object\n",
      "Sub_Object_Description    91603 non-null object\n",
      "Location_Description      162054 non-null object\n",
      "FTE                       126071 non-null float64\n",
      "Function_Description      342195 non-null object\n",
      "Facility_or_Department    53886 non-null object\n",
      "Position_Extra            264764 non-null object\n",
      "Total                     395722 non-null float64\n",
      "Program_Description       304660 non-null object\n",
      "Fund_Description          202877 non-null object\n",
      "Text_1                    292285 non-null object\n",
      "dtypes: category(9), float64(2), int64(1), object(14)\n",
      "memory usage: 55.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Brian/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.9047279995003435\n"
     ]
    }
   ],
   "source": [
    "# Import random forest classifer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Edit model step in pipeline\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.9135469364811692\n"
     ]
    }
   ],
   "source": [
    "# Import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Add model step to pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', RandomForestClassifier(n_estimators=15))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
