{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c496db30",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd19640c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./fra-eng/_about.txt  ./fra-eng/fra.txt\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "url = 'http://www.manythings.org/anki/fra-eng.zip'\n",
    "file = 'fra-eng.zip'\n",
    "#urlretrieve(url, file)\n",
    "# they don't like python user-agent - wants browser.\n",
    "\n",
    "with zipfile.ZipFile(file, \"r\") as f:\n",
    "    f.extractall('fra-eng')\n",
    "    \n",
    "!ls ./fra-eng/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a81ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "lines = []\n",
    "with open('fra-eng/fra.txt', \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        g = re.search('(.*\\t.*)\\t.*', line)\n",
    "        if g:\n",
    "            lines.append(g.group(1))\n",
    "            \n",
    "max_samples = 50000\n",
    "english_sentences = []\n",
    "french_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63854794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider only the first 50 lines of the dataset\n",
    "for i in range(max_samples):\n",
    "\t# Split each line into two at the tab character\n",
    "    eng_fra_line = str(lines[i]).split('\\t')\n",
    "    \n",
    "    # Separate out the English sentence \n",
    "    eng_line = eng_fra_line[0]\n",
    "    \n",
    "    # Append the start and end token to each French sentence\n",
    "    fra_line = '\\t' + eng_fra_line[1] + '\\n'\n",
    "    \n",
    "    # Append the English and French sentence to the list of sentences\n",
    "    english_sentences.append(eng_line)\n",
    "    french_sentences.append(fra_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2fe8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty set to contain the English vocabulary \n",
    "english_vocab = set()\n",
    "\n",
    "# Iterate over each English sentence\n",
    "for eng_line in english_sentences:\n",
    "  \n",
    "    # Convert the English line to a set\n",
    "    eng_line_set = set(eng_line)\n",
    "    \n",
    "    # Update English vocabulary with new characters from this line.\n",
    "    english_vocab = english_vocab.union(eng_line_set)\n",
    "\n",
    "# Sort the vocabulary\n",
    "english_vocab = sorted(list(english_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "411b33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty set to contain the French vocabulary \n",
    "french_vocab = set()\n",
    "\n",
    "# Iterate over each French sentence\n",
    "for fra_line in french_sentences:\n",
    "  \n",
    "    # Convert the French line to a set\n",
    "    fra_line_set = set(fra_line)\n",
    "    \n",
    "    # Update French vocabulary with new characters from this line.\n",
    "    french_vocab = french_vocab.union(fra_line_set)\n",
    "\n",
    "# Sort the vocabulary\n",
    "french_vocab = sorted(list(french_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08489f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to contain the character to integer mapping for English\n",
    "eng_char_to_idx = dict((char, idx) for idx, char in enumerate(english_vocab))\n",
    "\n",
    "# Dictionary to contain the integer to character mapping for English\n",
    "eng_idx_to_char = dict((idx, char) for idx, char in enumerate(english_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6020bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to contain the character to integer mapping for French\n",
    "fra_char_to_idx = dict((char, idx) for idx, char in enumerate(french_vocab))\n",
    "\n",
    "# Dictionary to contain the integer to character mapping for French\n",
    "fra_idx_to_char = dict((idx, char) for idx, char in enumerate(french_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "191e2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the length of the longest English sentence\n",
    "max_len_eng_sent = max([len(sentence) for sentence in english_sentences])\n",
    "\n",
    "# Find the length of the longest French sentence\n",
    "max_len_fra_sent = max([len(sentence) for sentence in french_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4fe76e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3-D zero vector for the input English data\n",
    "eng_input_data = np.zeros((len(english_sentences), max_len_eng_sent, len(english_vocab)), dtype='float32')\n",
    "\n",
    "# Create a 3-D zero vector for the input French data\n",
    "fra_input_data = np.zeros((len(french_sentences), max_len_fra_sent, len(french_vocab)), dtype='float32')\n",
    "\n",
    "# Create the target vector\n",
    "target_data = np.zeros((len(french_sentences), max_len_fra_sent, len(french_vocab)), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "479b81ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the 50 sentences\n",
    "for i in range(max_samples):\n",
    "    # Iterate over each English character of each sentence\n",
    "    for k, ch in enumerate(english_sentences[i]):\n",
    "        # Convert the character to one-hot encoded vector\n",
    "        eng_input_data[i, k, eng_char_to_idx[ch]] = 1\n",
    "        \n",
    "    # Iterate over each French character of each sentence\n",
    "    for k, ch in enumerate(french_sentences[i]):\n",
    "        # Convert the character to one-hot encoded vector\n",
    "        fra_input_data[i, k, fra_char_to_idx[ch]] = 1.\n",
    "\n",
    "        # Target data will be one timestep ahead and excludes start character\n",
    "        if k > 0:\n",
    "            target_data[i, k-1, fra_char_to_idx[ch]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "745de28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f88fb4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bosullivan/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# Create input layer\n",
    "encoder_input = Input(shape=(None, len(english_vocab)))\n",
    "\n",
    "# Create LSTM Layer of size 256\n",
    "encoder_LSTM = LSTM(256, return_state = True)\n",
    "\n",
    "# Save encoder output, hidden and cell state\n",
    "encoder_outputs, encoder_h, encoder_c = encoder_LSTM(encoder_input)\n",
    "\n",
    "# Save encoder states\n",
    "encoder_states = [encoder_h, encoder_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9917b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decoder input layer\n",
    "decoder_input = Input(shape=(None, len(french_vocab)))\n",
    "\n",
    "# Create LSTM layer of size 256\n",
    "decoder_LSTM = LSTM(256, return_sequences=True, return_state = True)\n",
    "\n",
    "# Save decoder output\n",
    "decoder_out, decoder_h , decoder_c = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
    "\n",
    "# Create a Dense layer with softmax activation\n",
    "decoder_dense = Dense(len(french_vocab), activation='softmax')\n",
    "\n",
    "# Save the decoder output\n",
    "decoder_out = decoder_dense(decoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19c86f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 77)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 103)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 342016      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  368640      input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 103)    26471       lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 737,127\n",
      "Trainable params: 737,127\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_out])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba8dcb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bosullivan/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/bosullivan/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.6753 - val_loss: 0.6847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f3bcba56128>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(x=[eng_input_data, fra_input_data], y=target_data,\n",
    "          \t\tbatch_size=64, epochs=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "274f6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder inference model\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Create decoder input states for inference\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ee312f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decoder output states for inference\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, initial_state=decoder_input_states)\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "# Create decoder dense layer\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states, outputs=[decoder_out] + decoder_states )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13040dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n"
     ]
    }
   ],
   "source": [
    "# Get encoder internal state by passing a sentence as input\n",
    "inp_seq = eng_input_data[0:1]\n",
    "states_val = encoder_model_inf.predict(inp_seq)\n",
    "\n",
    "# Seed the first character and get output from the decoder \n",
    "target_seq = np.zeros((1, 1, len(french_vocab)))\n",
    "target_seq[0, 0, fra_char_to_idx['\\t']] = 1  \n",
    "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "\n",
    "# Find out the next character from the Decoder output\n",
    "max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "sampled_fra_char = fra_idx_to_char[max_val_index]\n",
    "\n",
    "# Print the first character predicted by the decoder\n",
    "print(sampled_fra_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e22e1621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n"
     ]
    }
   ],
   "source": [
    "# Fill up target seq with the new char generated \n",
    "target_seq = np.zeros((1, 1, len(french_vocab)))\n",
    "target_seq[0, 0, max_val_index] = 1\n",
    "\n",
    "# Get decoder final states from last time\n",
    "states_val = [decoder_h, decoder_c]\n",
    "\n",
    "# Generate the next character\n",
    "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "\n",
    "# Map the prediction to char and print it\n",
    "max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "sampled_fra_char = fra_idx_to_char[max_val_index]\n",
    "\n",
    "print(sampled_fra_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adc403db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translated_eng_sentence(sent, french_vocab, decoder_h, decoder_c, decoder_model_inf, states_val, fra_idx_to_char):\n",
    "    \"\"\" Given a sentence, translate to French \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5fa1c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence: Go.\n",
      "French sentence: None\n",
      "English sentence: Go.\n",
      "French sentence: None\n",
      "English sentence: Go.\n",
      "French sentence: None\n",
      "English sentence: Hi.\n",
      "French sentence: None\n",
      "English sentence: Hi.\n",
      "French sentence: None\n",
      "English sentence: Run!\n",
      "French sentence: None\n",
      "English sentence: Run!\n",
      "French sentence: None\n",
      "English sentence: Run!\n",
      "French sentence: None\n",
      "English sentence: Run!\n",
      "French sentence: None\n",
      "English sentence: Run!\n",
      "French sentence: None\n"
     ]
    }
   ],
   "source": [
    "# Generate 10 French sentences from inp_seq\n",
    "for seq_index in range(10):\n",
    "  \n",
    "    # Get next encoded english sentence\n",
    "    inp_seq = eng_input_data[seq_index:seq_index+1]\n",
    "    \n",
    "    # Get the translated sentence\n",
    "    translated_sent = translated_eng_sentence(inp_seq)\n",
    "    \n",
    "    # Print the original English sentence\n",
    "    print('English sentence:', english_sentences[seq_index])\n",
    "    \n",
    "    # Print the translated French sentence\n",
    "    print('French sentence:', translated_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b079d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
